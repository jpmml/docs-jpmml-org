{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91db31a3-f86b-4388-8959-fbb6fc5c7436",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43790c8f-ca92-4477-bc1b-211eb113248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%AddDeps ml.dmlc xgboost4j-spark_2.12 3.1.1 --transitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa8b4a-a150-4042-9d51-e8af8f2fda9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%AddJar https://github.com/jpmml/jpmml-sparkml/releases/download/3.0.10/pmml-sparkml-example-executable-3.0.10.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a156a33-6016-46ce-8c71-dd334935a517",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13469681-b5e1-49fd-8de3-c8aa0812e6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def load_df(name: String): DataFrame = {\n",
    "    val df = spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        // Convert missing values to null references, not Double.NaN values\n",
    "        .option(\"nullValue\", \"N/A\")\n",
    "        .load(name)\n",
    "    df\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029c97a7-c0d0-4c62-8766-291474c347f8",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751d9236-6ee7-4563-90a0-e0b13db10b3f",
   "metadata": {},
   "source": [
    "## Option A: without missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9151f37e-69ca-429e-91ab-0e7081d08ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.feature.{StringIndexer, VectorAssembler}\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.types.FloatType\n",
    "import org.jpmml.sparkml.feature.VectorDensifier\n",
    "import ml.dmlc.xgboost4j.scala.spark.XGBoostRegressor\n",
    "\n",
    "val labelCol = \"mpg\"\n",
    "val catCols = Array(\"cylinders\", \"model_year\", \"origin\")\n",
    "val contCols = Array(\"acceleration\", \"displacement\", \"horsepower\", \"weight\")\n",
    "\n",
    "val df = load_df(\"Auto.csv\")\n",
    "    .withColumn(labelCol, col(labelCol).cast(FloatType))\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "val catIndexer = new StringIndexer()\n",
    "    .setInputCols(catCols)\n",
    "    .setOutputCols(catCols.map(_ + \"Indexed\"))\n",
    "\n",
    "val vecAssembler = new VectorAssembler()\n",
    "    .setInputCols(catIndexer.getOutputCols ++ contCols)\n",
    "\n",
    "val vecDensifier = new VectorDensifier()\n",
    "    .setInputCol(vecAssembler.getOutputCol)\n",
    "    .setOutputCol(vecAssembler.getOutputCol + \"Dense\")\n",
    "\n",
    "val featureTypes: Array[String] = catIndexer.getOutputCols.map(_ => \"c\") ++ contCols.map(_ => \"q\")\n",
    "\n",
    "val regressor = new XGBoostRegressor()\n",
    "    .setLabelCol(labelCol)\n",
    "    .setFeaturesCol(vecDensifier.getOutputCol)\n",
    "    .setFeatureTypes(featureTypes)\n",
    "    .setMaxDepth(3)\n",
    "    .setNumRound(11)\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "    .setStages(Array(catIndexer, vecAssembler, vecDensifier, regressor))\n",
    "\n",
    "val pipelineModel = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5035761-7470-4ed3-83c1-6ff8b9bcaf09",
   "metadata": {},
   "source": [
    "## Option B: with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce14a4b-ba0f-4dea-9a76-cc73bf8f0586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.feature.{StringIndexer, VectorAssembler}\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.types.FloatType\n",
    "import org.jpmml.sparkml.feature.{InvalidCategoryTransformer, VectorDensifier}\n",
    "import ml.dmlc.xgboost4j.scala.spark.XGBoostRegressor\n",
    "\n",
    "val labelCol = \"mpg\"\n",
    "val catCols = Array(\"cylinders\", \"model_year\", \"origin\")\n",
    "val contCols = Array(\"acceleration\", \"displacement\", \"horsepower\", \"weight\")\n",
    "\n",
    "val df = load_df(\"AutoNA.csv\")\n",
    "    .withColumn(labelCol, col(labelCol).cast(FloatType))\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "val catIndexer = new StringIndexer()\n",
    "    .setInputCols(catCols)\n",
    "    .setOutputCols(catCols.map(_ + \"Indexed\"))\n",
    "    // Pass through null values\n",
    "    .setHandleInvalid(\"keep\")\n",
    "\n",
    "// Drop the pseudo-category that was added by StringIndexer, because it will mess up XGBoost category indices\n",
    "val catIndexTransformer = new InvalidCategoryTransformer()\n",
    "    .setInputCols(catIndexer.getOutputCols)\n",
    "    .setOutputCols(catIndexer.getOutputCols.map(_ + \"Fixed\"))\n",
    "\n",
    "val vecAssembler = new VectorAssembler()\n",
    "    .setInputCols(catIndexTransformer.getOutputCols ++ contCols)\n",
    "    // Allow passed-through null values\n",
    "    .setHandleInvalid(\"keep\")\n",
    "\n",
    "val vecDensifier = new VectorDensifier()\n",
    "    .setInputCol(vecAssembler.getOutputCol)\n",
    "    .setOutputCol(vecAssembler.getOutputCol + \"Dense\")\n",
    "\n",
    "val featureTypes: Array[String] = catIndexer.getOutputCols.map(_ => \"c\") ++ contCols.map(_ => \"q\")\n",
    "\n",
    "val regressor = new XGBoostRegressor()\n",
    "    .setLabelCol(labelCol)\n",
    "    .setFeaturesCol(vecDensifier.getOutputCol)\n",
    "    .setFeatureTypes(featureTypes)\n",
    "    .setMaxDepth(3)\n",
    "    .setNumRound(11)\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "    .setStages(Array(catIndexer, catIndexTransformer, vecAssembler, vecDensifier, regressor))\n",
    "\n",
    "val pipelineModel = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fc9e77-7641-4e6c-ad17-79c162592d05",
   "metadata": {},
   "source": [
    "# Export to PMML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5867de6e-931c-4bda-80eb-d71e3dc04e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.jpmml.sparkml.PMMLBuilder\n",
    "import org.jpmml.sparkml.model.HasPredictionModelOptions\n",
    "import org.jpmml.xgboost.HasXGBoostOptions\n",
    "\n",
    "val pmmlBuilder = new PMMLBuilder(df.schema, pipelineModel)\n",
    "    // Apache Spark options\n",
    "    .putOption(HasPredictionModelOptions.OPTION_KEEP_PREDICTIONCOL, false)\n",
    "    // XGBoost options\n",
    "    .putOption(HasXGBoostOptions.OPTION_INPUT_FLOAT, true)\n",
    "    .putOption(HasXGBoostOptions.OPTION_COMPACT, false)\n",
    "\n",
    "println(pmmlBuilder.buildString)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
